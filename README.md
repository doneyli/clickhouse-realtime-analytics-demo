# ClickHouse Real-Time Analytics Demo

A **production-grade** demonstration of ClickHouse showcasing best practices for real-time analytics. This project demonstrates advanced ClickHouse features that make it the fastest open-source OLAP database for analytics workloads.

## ðŸŽ¯ What Makes This Demo Special

This isn't just a basic ClickHouse setup - it's a comprehensive showcase of **advanced optimizations**:

âœ… **LowCardinality & Compression** - 10-50x storage reduction
âœ… **Materialized Views** - 10-100x query speedup for aggregations
âœ… **Projections** - Multiple sort orders without data duplication
âœ… **Data Skipping Indices** - 10-100x faster filtered queries
âœ… **Dictionaries** - O(1) dimension lookups vs O(log n) JOINs
âœ… **TTL Policies** - Automatic data lifecycle management
âœ… **Refreshable MVs** - Complex analytics (rankings, cohorts, RFM)
âœ… **Optimized ORDER BY** - Based on actual query patterns

ðŸ“ **[View Complete Architecture Diagram â†’](ARCHITECTURE.md)** - Detailed Mermaid visualization of data flow

## ðŸš€ Features

- **ClickHouse Database**: High-performance columnar database optimized for analytics
- **Realistic Test Data**: 500K+ events, 10K users, 1K products, and 25K orders
- **Interactive Dashboard**: Modern web interface with real-time charts and analytics
- **AI Chat Assistant**: Natural language queries powered by Llama 3 (local Ollama)
- **Real-time Data Streaming**: Continuous data generation for live demos
- **Docker Setup**: Easy deployment with Docker Compose
- **RESTful API**: Backend API for data access and analytics
- **Responsive Design**: Beautiful, mobile-friendly dashboard

## ðŸ“Š Advanced Optimizations Implemented

### 1. **Optimized Table Schemas** (`init-scripts/01-create-tables.sql`)
- **LowCardinality** on all enum-like string columns (event_type, country, category, etc.)
  - Reduces memory usage by 10-50x
  - Enables dictionary encoding for compression
- **Compression Codecs**:
  - `Delta` encoding for timestamps (50-90% compression)
  - `T64` encoding for integers (better compression)
  - `ZSTD` for general-purpose compression
- **Optimized ORDER BY** based on actual query patterns:
  - Events: `(event_type, event_date, user_id, event_timestamp)` - optimized for event type filtering
  - Orders: `(status, order_date, user_id, order_timestamp)` - optimized for status filtering
  - Users: `(country, is_premium, user_id)` - optimized for geographic segmentation
- **Monthly Partitioning** with `toYYYYMM()` for better data management

### 2. **Materialized Views** (`init-scripts/01-create-tables.sql`)
Six pre-aggregated views for instant query response:
- **mv_product_revenue**: Product performance without JOINs (10-50x faster)
- **mv_user_funnel**: Conversion tracking with AggregatingMergeTree
- **mv_hourly_events**: Real-time dashboard with minimal latency
- **mv_country_stats**: Geographic analytics pre-aggregated
- **daily_user_activity**: Daily user metrics with SummingMergeTree

### 3. **Data Skipping Indices** (`init-scripts/02-add-indices.sql`)
14 indices across all tables for 10-100x faster filtered queries:
- **Bloom filters** for session_id lookups (O(1) vs O(n))
- **MinMax indices** for revenue, duration, age, spending (range queries)
- **Set indices** for country, event_type, status, category (IN/equality)

### 4. **Projections** (`init-scripts/03-add-projections.sql`)
8 projections for alternate sort orders without data duplication:
- **Events**: by country, by user, by session, pre-aggregated daily stats
- **Orders**: by product, by user
- **Users**: by spending, by registration date
- Query optimizer automatically selects the best projection

### 5. **TTL Policies** (`init-scripts/04-add-ttl.sql`)
Automatic data lifecycle management (100x more efficient than DELETE):
- **Events**: Keep detailed 30 days â†’ aggregate â†’ delete after 90 days
- **Orders**: Keep 1 year then delete
- **MVs**: Coordinated TTL with source tables
- Runs in background without blocking queries

### 6. **Refreshable Materialized Views** (`init-scripts/05-add-refreshable-mvs.sql`)
Complex analytics that can't be incrementally updated:
- **mv_top_products_ranking**: Product rankings with window functions
- **mv_customer_ltv**: Customer Lifetime Value with RFM analysis
- **mv_cohort_retention**: Retention analysis by registration cohort
- **mv_product_affinity**: Market basket analysis for recommendations
- **mv_daily_kpi_summary**: Comprehensive business metrics dashboard

### 7. **Dictionaries** (`init-scripts/06-add-dictionaries.sql`)
O(1) dimension lookups (10-100x faster than JOINs):
- **dict_users**: User dimension attributes
- **dict_products**: Product dimension attributes
- **dict_country_metadata**: Geographic enrichment
- **dict_category_metadata**: Category metadata with commission rates

### Database Schema
- **Users Table**: User profiles with demographics and spending data
- **Events Table**: User activity tracking (page views, clicks, purchases)
- **Products Table**: Product catalog with categories and pricing
- **Orders Table**: Transaction records with order details

### Analytics Features
- Daily user activity trends
- Event type distribution
- Revenue analytics by month
- Geographic user distribution
- Product performance metrics
- User segmentation analysis
- Real-time search and filtering

## ðŸ›  Prerequisites

- Docker and Docker Compose installed on your macOS
- At least 4GB of RAM available for containers
- Ports 8123, 9000, 3000, and 5001 available
- **Ollama** (installed locally) for AI chat features
- **Llama 3 model** pulled in Ollama: `ollama pull llama3`

## ðŸš€ Quick Start

### 1. Clone and Setup
```bash
cd ~/clickhouse-demo
```

### 2. Start ClickHouse Database
```bash
docker compose up -d clickhouse
```

Wait for ClickHouse to be fully ready (about 30-60 seconds).

### 3. Generate Test Data
```bash
# Install Python dependencies (in virtual environment)
source venv/bin/activate
pip install -r requirements.txt

# Generate substantial test data (this will take a few minutes)
python3 generate_data.py
```

### 4. Start the Web Application
```bash
docker compose up -d app
```

### 5. Access the Dashboard
Open your browser and navigate to: http://localhost:3000

### 6. (Optional) Start Real-time Data Streaming
```bash
# Start continuous data generation for live dashboard updates
./start_streaming.sh
```

This will add new events and orders every 30 seconds, perfect for demonstrating real-time analytics!

### 7. (Optional) Enable AI Chat Assistant
```bash
# Make sure Ollama is running locally and has llama3 model
ollama pull llama3

# Start AI chat service (uses your local Ollama)
cd ~/clickhouse-demo
source venv/bin/activate
CLICKHOUSE_HOST=localhost CLICKHOUSE_PORT=9000 CLICKHOUSE_USER=demo_user CLICKHOUSE_PASSWORD=demo_password CLICKHOUSE_DB=demo_db OLLAMA_HOST=localhost OLLAMA_PORT=11434 python3 chat_service.py
```

This adds an AI assistant that can answer questions about your data in natural language!

## ðŸ“ˆ Understanding the Data

### Data Volume
- **Users**: 10,000 user profiles
- **Events**: 500,000+ user interactions
- **Products**: 1,000 products across 10 categories  
- **Orders**: 25,000 purchase transactions

### Key Metrics Available
- **User Analytics**: Registration trends, geographic distribution, spending patterns
- **Event Analytics**: Daily activity, event type distribution, session analysis
- **Revenue Analytics**: Monthly revenue trends, order patterns
- **Product Analytics**: Top sellers, category performance
- **User Segmentation**: High/medium/low value customers

## ðŸ”§ Configuration

## ðŸŒŠ Real-time Data Streaming

The project includes a smart streaming script that continuously generates new data:

### Features
- **Automatic Data Generation**: New events and orders every 30 seconds
- **Size Management**: Automatically maintains database size limits
- **Realistic Data**: Generates authentic user interactions and transactions
- **Graceful Shutdown**: Stop with Ctrl+C without data corruption
- **Resource Safe**: Built-in protections against overwhelming the database

### Configuration
- **Stream Interval**: 30 seconds (configurable)
- **Max Events**: 10,000 total (auto-cleanup older data)
- **Max Orders**: 1,000 total (auto-cleanup older data)
- **Events per Batch**: 10 new events
- **Orders per Batch**: 3 new orders

### Usage
```bash
# Start streaming (from project directory)
./start_streaming.sh

# Or run directly
source venv/bin/activate
python3 stream_data.py
```

Perfect for demonstrating real-time analytics and dashboard updates!

## ðŸ¤– AI Chat Assistant

Query your ClickHouse data using natural language with Llama 3!

### Features
- **Natural Language Queries**: Ask questions in plain English
- **Smart SQL Generation**: AI writes optimized ClickHouse queries
- **Real-time Results**: Execute queries and see results instantly
- **Schema Awareness**: AI understands your database structure
- **Safety First**: Only SELECT queries allowed, no destructive operations
- **Beautiful Interface**: Modern chat UI with syntax highlighting
- **Local AI**: Uses your local Ollama installation for privacy and performance

### Example Questions
- "What are the top 5 countries by revenue?"
- "Show me daily user activity for the last week"
- "Which products are most popular?"
- "How many premium vs regular users do we have?"
- "What's our conversion rate from page views to purchases?"
- "Show me revenue trends by month"

### Technical Details
- **Model**: Llama 3 (via local Ollama)
- **Interface**: http://localhost:5001
- **Backend**: Flask + Local Ollama + ClickHouse
- **Safety**: Query validation and sanitization
- **Performance**: Optimized prompts for accurate SQL generation

### Setup Requirements
```bash
# Install Ollama (if not already installed)
# Visit: https://ollama.ai

# Pull the Llama 3 model
ollama pull llama3

# Start the chat service
cd ~/clickhouse-demo
source venv/bin/activate
CLICKHOUSE_HOST=localhost python3 chat_service.py
```

### ClickHouse Access
- **HTTP Interface**: http://localhost:8123
- **Username**: demo_user
- **Password**: demo_password
- **Database**: demo_db

### Manual ClickHouse Queries
You can connect directly to ClickHouse using the command line:

```bash
# Access ClickHouse client
docker exec -it clickhouse-demo clickhouse-client --user demo_user --password demo_password --database demo_db

# Example queries
SELECT count() FROM users;
SELECT event_type, count() FROM events GROUP BY event_type ORDER BY count() DESC;
SELECT toYYYYMM(order_date) as month, sum(total_amount) FROM orders WHERE status = 'completed' GROUP BY month ORDER BY month;
```

## ðŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Browser   â”‚â”€â”€â”€â–¶â”‚  Flask App      â”‚â”€â”€â”€â–¶â”‚   ClickHouse    â”‚
â”‚                 â”‚    â”‚  (Port 3000)    â”‚    â”‚  (Port 8123)    â”‚
â”‚  Dashboard UI   â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚  Charts & Tablesâ”‚    â”‚  REST API       â”‚    â”‚  Analytics DB   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components
- **ClickHouse**: Columnar database for analytics workloads
- **Flask**: Python web framework serving the API and dashboard
- **Plotly.js**: Interactive charting library
- **Bootstrap**: Responsive CSS framework
- **Docker**: Containerization for easy deployment

## ðŸ“ API Endpoints

The Flask application provides REST API endpoints showcasing optimized queries:

### Core Endpoints
- `GET /api/stats` - Basic database statistics
- `GET /api/daily-events` - Daily event trends
- `GET /api/event-types` - Event type distribution
- `GET /api/top-countries` - User distribution by country
- `GET /api/revenue-by-month` - Monthly revenue data
- `GET /api/user-segments` - User segmentation analysis
- `GET /api/search` - Search users or products

### Optimized Endpoints (New!)
- `GET /api/top-products` - **Uses mv_product_revenue MV** (10-50x faster)
- `GET /api/conversion-funnel` - **Uses mv_user_funnel MV** with State functions
- `GET /api/hourly-activity` - **Uses mv_hourly_events MV** for real-time dashboards

These endpoints demonstrate query optimization techniques in `app.py`.

## ðŸ›  Development

### Running Components Separately

**ClickHouse Only:**
```bash
docker-compose up -d clickhouse
```

**Flask App Locally:**
```bash
export CLICKHOUSE_HOST=localhost
export CLICKHOUSE_PORT=9000
export CLICKHOUSE_USER=demo_user
export CLICKHOUSE_PASSWORD=demo_password
export CLICKHOUSE_DB=demo_db

python3 app.py
```

### Customizing Data Generation

Edit `generate_data.py` to modify:
- Number of users, events, products, orders
- Data patterns and distributions
- Date ranges for historical data
- Geographic distributions

### Adding New Analytics

1. Add new SQL queries to `app.py`
2. Create new API endpoints
3. Update the dashboard HTML with new charts
4. Use Plotly.js for visualizations

## ðŸ” Comprehensive Query Examples

See **`examples/sample_queries.sql`** for 9 sections of comprehensive examples:

1. **Basic Analytics** - LowCardinality, indices, optimized ORDER BY
2. **Materialized Views** - 10-100x faster aggregations
3. **Projections** - Automatic selection of best sort order
4. **Dictionaries** - O(1) dimension lookups
5. **Refreshable MVs** - Rankings, CLV, cohorts, market basket
6. **Window Functions** - row_number, percent_rank, ntile
7. **Advanced Patterns** - Funnels, sessionization, PREWHERE
8. **Performance Analysis** - EXPLAIN, query logs, optimization checks
9. **Checking Optimizations** - Compression ratios, index usage, table sizes

### Quick Examples

```sql
-- Query materialized view instead of JOIN (10-50x faster!)
SELECT p.product_name, SUM(mv.total_revenue) as revenue
FROM mv_product_revenue mv
JOIN products p ON mv.product_id = p.product_id
WHERE mv.status = 'completed'
GROUP BY p.product_id, p.product_name
ORDER BY revenue DESC LIMIT 20;

-- Use dictionary for dimension enrichment (O(1) lookup)
SELECT
    e.event_id,
    dictGet('dict_users', 'country', e.user_id) as country,
    dictGet('dict_users', 'is_premium', e.user_id) as is_premium,
    e.event_type, e.revenue
FROM events e WHERE e.event_date = today() LIMIT 100;

-- Conversion funnel with AggregatingMergeTree
SELECT
    sumMerge(page_views) as page_views,
    sumMerge(purchases) as purchases,
    round(purchases * 100.0 / page_views, 2) as conversion_rate
FROM mv_user_funnel
WHERE event_date >= today() - INTERVAL 30 DAY;

-- Customer Lifetime Value from Refreshable MV
SELECT user_id, username, ltv_segment, lifetime_value,
       round(recency_percentile * 100, 1) as recency_score
FROM mv_customer_ltv
WHERE ltv_segment = 'High Value'
ORDER BY lifetime_value DESC LIMIT 50;

-- Check which projection is used
EXPLAIN SELECT * FROM events WHERE user_id = 1234 ORDER BY event_timestamp;
-- Look for "Projection Name: proj_by_user"
```

**Run these queries in ClickHouse client:**
```bash
docker exec -it clickhouse-demo clickhouse-client --user demo_user --password demo_password --database demo_db < examples/sample_queries.sql
```

## ðŸ§¹ Cleanup

To stop and remove all containers:
```bash
docker-compose down -v
```

To remove all data and start fresh:
```bash
docker-compose down -v
docker volume prune
```

## ðŸŽ“ Learning Resources

### Understanding the Optimizations

Each init-script file includes extensive documentation:

- **01-create-tables.sql** - Table schemas, compression, ORDER BY strategy
- **02-add-indices.sql** - Data skipping indices (bloom filter, minmax, set)
- **03-add-projections.sql** - Alternate sort orders without duplication
- **04-add-ttl.sql** - Automatic data lifecycle management
- **05-add-refreshable-mvs.sql** - Complex analytics (rankings, CLV, cohorts)
- **06-add-dictionaries.sql** - Fast dimension lookups

### Why ClickHouse for Real-Time Analytics?

**Performance Characteristics:**
- **Query Speed**: 10-100x faster than traditional databases for analytics
- **Storage Efficiency**: 80-90% compression with codecs
- **Scalability**: Linear scalability to petabyte scale
- **Real-time**: Instant data availability after INSERT

**Key Differentiators:**
1. **Columnar Storage**: Only read columns needed for query
2. **Vectorized Execution**: SIMD operations on data chunks
3. **Data Skipping**: Skip irrelevant data granules with indices
4. **Materialized Views**: Pre-aggregated results update automatically
5. **Projections**: Multiple physical layouts for optimal access patterns

**When to Use ClickHouse:**
âœ… Real-time analytics dashboards
âœ… Log and event data analysis
âœ… Time-series data
âœ… High-throughput INSERT workloads
âœ… Complex aggregations on billions of rows

**When NOT to Use:**
âŒ Transactional workloads (use PostgreSQL, MySQL)
âŒ Frequent updates/deletes (mutations are expensive)
âŒ Small datasets (<1M rows) - overhead not worth it
âŒ Ad-hoc queries on non-optimized columns

## ðŸŽ¯ Next Steps / Future Enhancements

This demo already implements production-grade optimizations. Consider extending with:

- **Kafka Integration**: Real-time streaming ingestion
- **Distributed Tables**: Sharding across multiple nodes
- **Replication**: High availability with ReplicatedMergeTree
- **External Data Sources**: Query MySQL, PostgreSQL via External Tables
- **Custom Dashboards**: Build specific visualizations for your use case
- **Query Result Cache**: Cache frequent query results
- **Machine Learning**: Use ClickHouse for feature engineering

## ðŸ“š Learning Resources

- [ClickHouse Documentation](https://clickhouse.com/docs)
- [ClickHouse SQL Reference](https://clickhouse.com/docs/en/sql-reference/)
- [Performance Optimization Guide](https://clickhouse.com/docs/en/operations/performance/)
- [Flask Documentation](https://flask.palletsprojects.com/)
- [Plotly.js Documentation](https://plotly.com/javascript/)

## ðŸ› Troubleshooting

**ClickHouse won't start:**
- Check if ports 8123 and 9000 are available
- Ensure Docker has enough memory allocated
- Check logs: `docker-compose logs clickhouse`

**Data generation fails:**
- Wait longer for ClickHouse to be fully ready
- Check network connectivity: `curl http://localhost:8123/ping`
- Verify credentials in the configuration

**Dashboard shows no data:**
- Ensure data generation completed successfully
- Check Flask app logs: `docker-compose logs app`
- Verify ClickHouse contains data: `docker exec -it clickhouse-demo clickhouse-client --user demo_user --password demo_password -q "SELECT count() FROM demo_db.users"`

**Performance issues:**
- ClickHouse performs better with more RAM
- Consider adjusting Docker memory limits
- Monitor query performance in ClickHouse logs

---

ðŸŽ‰ **Enjoy exploring the power of ClickHouse for analytics!**
